### 概述

持续交付从“持续集成维度”、“质量保证维度”、“环境配置管理维度”、“持续部署及发布维度”5个维度考察其成熟度。

每个维度分为“原始”、“入门”、“初级”、“中等”、“高级”和“专家”6个级别。

目前在各个维度上，行业的平均水平集中在“入门”和“新手”两个级别。

评估时各级别之间不能越级，就是说即使“新手”中个别条目已经做到了，但如果“入门”中有条目没有做到，也只能评为“入门”级。

该模型的主要目的是为了更好地帮助团队认识现状，同时了解改进的方向。该模型是对持续集成主要维度的简单衡量，背后可能有一些并不适合团队的假设。

由于技术和项目的差异性，不同团队达到同一级别需要付出的努力可能差异很大，获得的收益也有区别，因此不适宜利用此模型在团队间进行比较。

#### 持续集成维度

通过下面问题进一步了解团队的构建现状：

1. 如何管理编译依赖？
2. 如何管理模块版本依赖？
3. 编译的时间是多长？
4. 每个人的构建环境和方法是否一致？
5. 自动化构建的具体步骤？
6. 自动化构建失败的主要原因是什么？
7. 是否使用了Build Grid？如何用的？
8. 如何管理Grid中的各台机器？如何同步和更新环境？
9. 本地构建的内容？什么时候运行？要多长时间？
10. 使用那个CI平台？如何配置的？
11. 如何调整构建中包含的内容？

##### 原始：

1. 手动方式构建。
2. 构建时间很长，且经常在核心的功能级别发生失败。

##### 入门：

1. 使用版本控制(git、subversion等)。
2. 构建脚本。
3. 定时构建(每日构建或每周构建等)。
4. 专用构建服务器。
5. 手动更新软件版本。

##### 新手：

1. 每次代码提交后即运行包含自动化测试的构建过程。
2. 构建过程中的所有代码、测试代码、构建脚本、部署脚本和环境配置等都统一管理在版本控制库中。
3. 只打包一次，能部署到任何环境。
4. 依赖管理仓库。
5. 构建过程版本化。
6. 构建结果被有效通知团队。
7. 团队所有人都知道代码提交流程(七步提交法)。
8. 专人维护构建环境。

##### 中等

1. 能在本地和集成系统中运行单元测试、集成测试和功能测试。
2. 收集构建的状态和度量数据，对所有人可见。
3. 构建失败立即修复，否则回滚。
4. 基础的持续交付流水线。
5. 充分利用多台机器运行多个构建。

##### 高级

1. 频繁提交，保证持续交付流水线不失败。
2. 运行时间超过限制即构建失败。
3. 在新的环境中，能一键构建完整的开发测试环境并打包部署。
4. 记录构建的趋势指标，持续改进。

##### 专家

1. 代码质量可达到随时部署产品环境。

#### 质量保证维度

在其它纬度中也有一些与测试相关的条目，可参考。

##### 原始：

##### 入门：

1. 有部分自动化功能测试
2. 在项目后期集中测试
3. 利用缺陷跟踪系统管理缺陷
4. 仅有少量的单元测试，尚未发挥明显作用。

##### 新手：

1. 最小工作单元包含手工测试。
2. 积累一定量的单元测试，团队已从中受益。
3. 构建时运行自动回归测试。
4. 有人工参与的提测过程。
5. 自动化功能测试具备一定数量并起到了一定保障作用。
6. 自动化功能测试全集频繁运行，不少于一天一次。

##### 中等：

1. 最小工作单元包含自动化测试工作。
2. 不同层级的自动化测试发挥质量保障作用。
3. 静态代码检查及相关举措
4. 可以在构建中推荐提测版本
5. 自动化提测

##### 高级：

1. 普遍的单元测试，发挥良好效果。
2. 自动化测试覆盖率较高，测试工作被有效分散在开发阶段。
3. 手工测试大部分属于探索性测试。

##### 专家：

1. 100%单元测试覆盖率。
2. 自动化测试提供信心十足的质量保证，构建成功后即自动部署。

通过下面问题进一步了解团队的构建现状：

1. 有那些级别的测试，现状如何？
2. 提测流程是怎么样的？需要多长时间？有多少人工参与？
3. 集中的测试阶段占整个项目周期的比例？
4. QA和RD的合作流程是怎么样的？
5. 有那些自动化测试，数量和质量分别如何？
6. 自动化测试一般是什么时候写的，谁维护，怎么管理和运行的？
7. 什么时间，如何做回归测试？
8. 自动化验收测试、性能测试以及安全性测试现状？
9. 应用了那些静态代码检查，怎么用的？
10. 单元测试的覆盖率如何？
11. 什么时机，做那些人工测试？
12. 如何选择对哪个构建做测试？

####  持续部署及发布维度

##### 原始：

1. 部署流程文档化。

##### 入门：

1. 有辅助脚本支持的手工部署
2. 依据文档的人工上线流程

##### 新手：

1. 完整的部署脚本支持
2. 向测试环境的标准化部署
3. 通过平台的半自动上线流程

##### 中等：

1. 选择指定的构建产出进行自动部署
2. 可以推荐某个构建为上线候选版本

##### 高级：

1. 向生产环境中一键发布，一键回滚。
2. 向生产线部署后的自动化验证。

##### 专家：

1. 一键恢复生产环境

通过下面问题进一步了解团队的部署及发布现状：

1. 上线流程是什么样的？
2. 是否需要通过work文档，邮件或是一个在线系统传递上线步骤或参数？
3. 如何监测部署的质量？
4. 如何做回滚操作？
5. 如何向开发环境部署？有那些步骤？需要多少人工参与？
6. 如何向测试环境部署？有那些步骤？需要多少人工参与？
7. 有那些用于部署的脚本？
8. 团队成员各自的部署环境有什么区别？
9. 如何选择合适的构建做部署？

#### 环境配置管理维度

##### 原始：

##### 入门：

1. 使用版本管理工具
2. 使用私有分支
3. 功能测试Case在本地管理
4. 生产代码被版本管理
5. 每天提交代码，并写Comment。

##### 新手：

1. 应用在最小工作单元完成后即可集成的分支策略
2. 所有构建、测试及部署脚本都被版本管理
3. 所有测试代码和数据被集中管理
4. 测试环境中的应用配置被版本管理
5. 代码签入的Comment清楚达意，含有必要的Metadata，比如Bug号等。
6. RD使用完全标准的开发环境，没有仅供自己使用的脚本、数据或测试环境等。

##### 中等：

1. 测试代码与生产代码同源
2. 生产环境中的应用配置被版本管理
3. 持续集成平台运行的脚本被版本管理，无需登录平台修改脚本。
4. 每次构建均有版本追溯，无需重新从源码构建历史版本。
5. Qa使用标准的开发测试环境
6. 数据库被版本管理
7. 功能测试数据被版本管理

##### 高级：

1. 没有仅在团队成员本地保存的任何项目资产。
2. RD和QA使用一致的开发测试环境。

##### 专家：

1. 开发、测试、生产环境被版本管理，可以一键克隆。
2. 所有测试数据被版本管理

通过下面问题进一步了解团队的配置管理现状：

1. 使用何种分支管理策略？
2. 团队成员签入代码的频率和内容，Comment的质量如何？
3. 一个RD在全新的机器上建立完整的工作环境要经历那些步骤？
4. 一个QA在全新的机器上建立完整的工作环境要经历那些步骤？
5. RD和QA的工作环境有什么区别？
6. RD之间的工作环境有什么区别？
7. Qa之间的工作环境有什么区别？
8. 测试数据是怎么管理的？
9. RD是否在沙盒中开发？如果不是，有那些依赖？
10. 本地和平台构建脚本是如何管理的？
11. 测试环境和生产环境的应用配置是如何管理的？
12. 测试工具和测试Case是如何管理的？
13. 测试运行的环境依赖有那些？是否每个人在自己的工作环境中都可以方便运行？
14. 所有团队成员是否使用相同的脚本做本地构建？
15. 团队成员有那些私有的代码，脚本和预先部署的环境？

#### 团队习惯维度

##### 入门：

1. 至少有一个人随时知晓构建状态
2. 阶段性代码提交习惯
3. 专人维护持续集成平台和脚本

##### 新手：

1. 专人看护平台构建状态
2. 最小工作单元完成后即时合并到目标分支
3. 所有人知晓当前的构建状态
4. 构建失败后被及时修复或回滚，失败期间没人提交与修复构建无关的代码。
5. 签入前做本地自动化验证
6. 团队成员签入代码前做相同的本地验证
7. 失败构建不过夜

##### 中等：

1. 失败构建修复时间少于半个小时。
2. 由提交人负责修复失败构建，每个人都关注构建状态。
3. 团队成员都写较为全面的单元测试
4. 每人每天向目标分支做至少一次对最小工作单元的有效提交。
5. 团队清楚持续集成平台和脚本内容，每个人都可以维护。

##### 高级：

1. 交付团队全员对持续集成平台的稳定构建负责。
2. 交付团队全员负责持续集成脚本开发。

##### 专家：

1. 1小时左右向目标分支做一次对最小工作单元的有效提交，且很少发现构建失败。

通过下面问题进一步了解团队的部署及发布现状：

1. RD如何定义自己工作完成的含义？
2. 团队签入代码的规范是什么？
3. 是否在签入代码前运行本地构建？
4. 如何确保失败的平台构建有人处理？是签入代码的人处理吗？
5. 构建失败的频率是多少？
6. 团队中谁维护自动化构建脚本？
7. 团队中谁维护CI平台？CI平台有那些权限控制？
8. 团队如何提高每个人的单元测试水平？
